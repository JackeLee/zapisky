\chapter{Algorithms}
In this chapter we introduce two algorithms for searching minimax trees
AlphaBeta search and Monte Carlo Tree Search (MCTS). Both algorithms became
successful and dominating in some field. AlphaBeta search in game of Chess and
MCTS in game of Go. (TODO reformulate)

The basic algorithm to search game tree is Minimax search. It is searching
minimax tree in depth first search order and therefore the search do not ends
until all nodes at certain depth are explored. It is quite noneffective with
huge branching factor which Arimaa, Chess or Go have.

\section{Description of the AlphaBeta search}
AlphaBeta search has grown in popularity in game of Chess. Until now all
successful arimaa bots were using it with various enchants. AlphaBeta search is
natural optimisation of Minimax search. The main purpose of algorithm is to
reduce number of branches and nodes to be visited.

During the search we are trimming bounds (window) of the best minimax value.
The pseudocode of the algorithm is shown in~\ref{alg:alphabeta}.

\lstset{language=Python, caption=Pseudocode of the AlphaBeta search, label=alg:alphabeta}
\begin{lstlisting}
alphabeta (node, depth, alpha, beta):
    if  depth = 0 or node is a terminal node:
        return evaluate(node)
    if  is maximizing node:
        for each child of node
            score = alphabeta(child, depth-1, alpha, beta)
            alpha = max(alpha, score)

            if beta <= alpha: break  # Beta cut-off
        return alpha
    else:
        for each child of node:
            score = alphabeta(child, depth-1, alpha, beta)
            beta = min(beta, score)

            if beta <= alpha: break  # Alpha cut-off
        return beta
\end{lstlisting}

In maximizing nodes we are improving lower estimate (alpha) of the minimax
value and in minimizing nodes the upper estimate (beta). If a value from a
child forces these bounds to meet we know that better approximation of this
node is not possible and so we return our estimate, we say we pruned/cut-off
the search in node.

As can be proven, if AlphaBeta finds solution for depth $n$, then it is the
best solution in MiniMax tree for depth $n$~\cite{knuth:alphabeta}. In optimal
case instead of examining $\mathcal O(b^d)$ nodes is only examined $\mathcal
O(b^{d/2})$ of them, where $b$ is branching factor of the game and $d$ is
searched depth~\cite{ZHONG}.

A lot of time is spared if the pruning children of nodes are listed as first.
However in the worst case if children are sorted in opposite order to the
optimal, the whole minimax tree is searched. Consequently it is very important
to have nodes well ordered.


\section{Description of the Monte Carlo Tree search}
Monte Carlo methods was formerly used for approximation of mathematical,
physical, biological and others processes where full calculation would be
difficult or even impossible. For example in mathematics it is used for numeric
integration or estimate the value of~$\pi$. In games without perfect
information, such as poker, backgammon, or scrabble, using randomness has shown
to be beneficial too~\cite{MonteCarloMethod, MonteCarloGo}.

In game of Go was always a huge troublesome to build efficient evaluation
function. Unlike poker or backgammon, Go is a game with perfect information and
hence it is not so natural to use Monte Carlo methods. The first attempts to
use random approach as evaluating approximation of the Go position were in
1993. We present generalised Bernd's algorithm~\cite{BERND,KOZELEK}:

\begin{enumerate}
\item Play random game from given position to the end, with one exception,
	choose only steps not filling eyes. At the end of simulation count in the
	result of simulation for the first step played.
\item If there is time left go to 1.
\item Choose move with highest ratio between number times the move won when it
	  was played and the number of times it was played.
\end{enumerate}

However Bernd showed that after some time his algorithm indicates no further
improvement. It was necessary to realize that the main problem is that
algorithm searches all branches with the same probability. It should give more
attention to more promising branches. Question how to find fine balancing
between exploration and exploitation is classical mathematical Multi-armed
Bandit problem.

\subsection{Bandit Problem}
A $K$-armed bandit, is slot machine with $K$ arms. When arm is drawn it
produces reward. Distribution of each arm reward is independent on other arms
and previous draws of this arm. The task is to choose best strategy to maximize
sum of reward through iterative plays~\cite{MoGo,MultiarmedBandit}.

In~\cite{MultiarmedBandit} is presented thee UCB1 algorithm (where UCB stands for Upper Confidence Bounds) for Bandit Problem:

\begin{enumerate}
\item Play each arm of the bandit once.
\item Play arm maximizing the formula $\overline X_i + \sqrt{2 \log n \over n_i}$,
	  where $\overline X_i$ is average value of the arm $i$, $n$ is number
	  of games that were played by parent of the $i$ and $n_i$ is number of
	  games played with arm $i$.
\end{enumerate}


\subsection{UCT algorithm}
UCT is abbrevation for  Upper Confidence bound to Trees. Shortly described it
is the UCB algorithm applied to minimax trees. The main idea is to consider
each node of minimax tree as multiarmed bandit problem and each child of the
node as independent arm~\cite{MoGo}.


\subsection{MCTS}
Monte Carlo Tree search is generic best-first-search algorithm applied to
trees which uses a random simulation as evaluation scheme. It consist of four
steps which are repeated as long as there is time left. We start with a tree
containing only a root node representing starting
position~\cite{progressive-strategies,KOZELEK}.

The four mentioned steps are explained in the following list:

\begin{enumerate}
\item \emph{Selection:} Until a leaf node is reached a child of node is
recursively selected by using best-first manner. The node selection is usually
handled by UCT algorithm.
\item \emph{Expansion:} The selected leaf's children are generated.
\item \emph{Simulation} (also called \emph{playout}): Random game is played
from given position to certain depth.
\item \emph{Backpropagation:} The result of simulated games is stored into all
nodes visited during Selection phase in this iteration of algorithm.
\end{enumerate}

Classical approach of the Simulation phase in Go bot programming is to generate
only steps not filling eyes and to stop playout when the end of game is
reached (which is not possible in Arimaa)~\cite{MoGo}.

The result of the Simulation phase is computed from the last position of the
simulation by a evaluation function. The probabilistic attitude of UCT/UCB1
requires the value of evaluation to be scaled in interval
$[-1,1]$~\cite{KOZELEK}.

A pseudocode of the full algorithm is shown in~\ref{alg:mcts} (inspired
by~\cite{MoGo}).

\lstset{language=Python, caption=Pseudocode of the MonteCarlo Tree Search, label=alg:mcts}
\begin{lstlisting}
playOneIteration():
    node[0] = rootNode
    i = 0
    while is_not_leaf(node[i]):
        node[i+1] = descendByUCB1(node[i])
        i = i + 1

    expandNode(node[i])
    node[i].value = getValueByMonteCarlo(node[i])

    for j in {0, ..., i-1}:
        updateValue(node[j],node[i].value)

while is_time():
    playOneIteration()

print descendByUCB1(rootNode)
\end{lstlisting}

We modified Kozelek's UCB exploration formula to form:
	$$
	\overline X_i + c \sqrt{\frac{log~n}{n_i}} + \frac{h_i}{n_i} + \frac{hh_i}{n_i}
	$$
where $\overline X_i + c \sqrt{\frac{log~n}{n_i}}$ is a generalised UCB1
formula, $h_i$ is heuristics evaluated for step leading to this position, and
$hh_i$ is history heuristic value.

One of the negatives of UCT algorithm is that UCB1 formula itself presumes the
involved random variables are identically distributed and independent, which is
not true in UCT~\cite{MoGo}. It was shown that the probability of selecting the
correct move quickly converges to 100\%~\cite{UCTanalysis}.

